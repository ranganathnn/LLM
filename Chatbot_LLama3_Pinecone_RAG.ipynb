{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m83trQLwf5YI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "from pinecone import Pinecone\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"xxxxx\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"xxxxx\"\n",
        "\n",
        "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "SIz-EpXEf7_X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "\n",
        "index_name = \"read-file-chatbot\"\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatGroq(model_name=\"llama3-8b-8192\"),\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5el1zk91JE9",
        "outputId": "8a3362fd-2fe2-4e0b-c15b-448ceb6f0c81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4eff38bc8215>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-3-4eff38bc8215>:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_html(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch URL {url} - Status Code: {response.status_code}\")\n",
        "\n",
        "def extract_text_from_html(html_content, num_pages=3):\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    extracted_text = [p.get_text() for p in paragraphs]\n",
        "\n",
        "    text_chunk = \" \".join(extracted_text[:num_pages * 500])\n",
        "\n",
        "    return text_chunk\n",
        "\n",
        "def index_text_in_pinecone(text):\n",
        "    \"\"\"Splits text and indexes it in Pinecone.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    documents = text_splitter.create_documents([text])\n",
        "\n",
        "    # Store in Pinecone\n",
        "    vectorstore.add_texts([doc.page_content for doc in documents])\n",
        "    print(\"Text indexed successfully!\")\n",
        "\n",
        "def ask_question(query):\n",
        "    \"\"\"Queries the chatbot with conversation memory.\"\"\"\n",
        "    response = qa_chain.run({\"question\": query})\n",
        "    return response"
      ],
      "metadata": {
        "id": "jm6QiH4R1I43"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Fetch HTML content\n",
        "    url = \"https://arxiv.org/html/2412.19437v1\"\n",
        "    html_content = fetch_arxiv_html(url)\n",
        "\n",
        "    # Extract text from first 3 pages\n",
        "    extracted_text = extract_text_from_html(html_content)#, num_pages=3)\n",
        "\n",
        "    # Index text in Pinecone\n",
        "    index_text_in_pinecone(extracted_text)\n",
        "\n",
        "    # Chat loop with memory\n",
        "    print(\"\\nChatbot ready! Type 'exit' to stop.\\n\")\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "        response = ask_question(query)\n",
        "        print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ngpnyIiWDd",
        "outputId": "65cfcf01-a802-4609-8f6b-dcec2687ef0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Text Preview:\n",
            " 001 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.\n",
            "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE a\n",
            "Text indexed successfully!\n",
            "\n",
            "Chatbot ready! Type 'exit' to stop.\n",
            "\n",
            "You: What is the main contribution of the paper?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5a83457919be>:33: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa_chain.run({\"question\": query})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I don't know the specific paper being referred to, but based on the context provided, it appears to be related to a neural network architecture, possibly a gated fusion network. The notation and symbols used suggest that the paper may be discussing a specific type of expert-based model, where experts are routed or shared based on input tokens.\n",
            "\n",
            "Without more specific information about the paper, I'm unable to determine the main contribution of the paper. If you could provide more context or details about the paper, I'd be happy to try and help answer your question.\n",
            "You: Can you explain that in simpler terms?\n",
            "Chatbot: I'm not entirely sure, as the provided context appears to be a technical description of the paper in a mathematical notation. However, I can try to simplify it:\n",
            "\n",
            "The paper seems to be about improving the efficiency of a specific type of artificial intelligence model called a transformer. Specifically, it focuses on reducing the memory usage of the model during inference (i.e., when it's making predictions).\n",
            "\n",
            "It does this by compressing the \"keys\" and \"values\" used in the model, which are important components of the transformer's attention mechanism. By compressing these components, the model can reduce the amount of memory it needs to store and process, making it more efficient and potentially allowing it to handle larger inputs or run faster on devices with limited memory.\n",
            "You: How does it compare to previous methods?\n",
            "Chatbot: I don't know. The provided text does not mention anything about a specific method for reducing memory usage in transformer models, nor does it compare it to previous methods. It seems to be discussing general research goals and approaches for improving the capabilities of transformer models.\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vf0fQ456iWrG"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}